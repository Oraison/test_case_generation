{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,tokenizer):\n",
    "    sources=[]\n",
    "    targets=[]\n",
    "    \n",
    "    with open(f'data/{path}.jsonl', encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            obj=json.loads(line)\n",
    "            # source=obj['description']+tokenizer.sep_token+obj['solutions']\n",
    "            source = obj['solutions']\n",
    "            \n",
    "            for t in obj['test_cases']:\n",
    "                sources.append(source)\n",
    "                targets.append(t+tokenizer.eos_token)\n",
    "            for t in obj['private_tests']:\n",
    "                sources.append(source)\n",
    "                targets.append(t+tokenizer.eos_token)\n",
    "            if idx>50000:\n",
    "                break\n",
    "\n",
    "    df=pd.DataFrame()\n",
    "    df['source']=sources\n",
    "    df['target']=targets\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"MODEL\": \"Salesforce/codet5-base\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 16,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 16,  # validation batch size\n",
    "    # \"TRAIN_EPOCHS\": 10,  # number of training epochs\\\n",
    "    \"TRAIN_EPOCHS\" : 2,\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 32,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_params[\"MODEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.summ_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    print(epoch, \"th train start\")\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        if i % 50==0:\n",
    "            aver_loss = total_loss / 50\n",
    "            total_loss = 0\n",
    "            print( int(i/50), ': ', aver_loss)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, \"th train end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "  print(\"validate start\")\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  sources = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=5,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "           \n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          source = [tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=True) for i in ids]\n",
    "          sources.extend(source)\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "  print(\"validate end\")\n",
    "  return predictions, actuals,sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T5Trainer(\n",
    "      output_dir=\"./outputs/\"\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    # dataframe = dataframe[[source_text, target_text]]\n",
    "    dataframe = load_data('seq_data2',tokenizer)\n",
    "    source_text='source'\n",
    "    target_text='target'\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.8\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = YourDataSetClass(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    )\n",
    "    print('starting training')\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "        predictions, actuals,sources = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals,\"Source Text\":sources})\n",
    "        final_df.to_csv(os.path.join(output_dir, f\"predictions_train{epoch}.csv\"))\n",
    "    # Saving the model after training\n",
    "        path = os.path.join(output_dir, f\"model_files{epoch}\")\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        # final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "        # final_df.to_csv(os.path.join(output_dir, f\"predictions_valid{epoch}.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "0 th train start\n",
      "0 :  0.0\n",
      "1 :  2.5239650917053225\n",
      "2 :  1.9241860795021057\n",
      "3 :  1.766630527973175\n",
      "4 :  1.6654189467430114\n",
      "5 :  1.5833751726150513\n",
      "6 :  1.5387217116355896\n",
      "7 :  1.5088592064380646\n",
      "8 :  1.448569676876068\n",
      "9 :  1.376787269115448\n",
      "10 :  1.294640702009201\n",
      "11 :  1.3036453068256377\n",
      "12 :  1.2786094951629638\n",
      "13 :  1.2480491042137145\n",
      "14 :  1.192383757829666\n",
      "15 :  1.1432636165618897\n",
      "16 :  1.1611981451511384\n",
      "17 :  1.1755417084693909\n",
      "18 :  1.1054226624965668\n",
      "19 :  1.0533425652980803\n",
      "20 :  1.0152714610099793\n",
      "21 :  1.0093003904819489\n",
      "22 :  1.0213669979572295\n",
      "23 :  0.9727706289291382\n",
      "24 :  1.0079121923446654\n",
      "25 :  0.9209653878211975\n",
      "26 :  0.8935674726963043\n",
      "27 :  0.8904398155212402\n",
      "28 :  0.8650844264030456\n",
      "29 :  0.8707194399833679\n",
      "30 :  0.9252889358997345\n",
      "31 :  0.8645593106746674\n",
      "32 :  0.8323616290092468\n",
      "33 :  0.8033535647392273\n",
      "34 :  0.7944408869743347\n",
      "35 :  0.8144040083885193\n",
      "36 :  0.7595306730270386\n",
      "37 :  0.7465743374824524\n",
      "38 :  0.7727706146240234\n",
      "39 :  0.7617390215396881\n",
      "40 :  0.7245769035816193\n",
      "41 :  0.7295385587215424\n",
      "42 :  0.6942407584190369\n",
      "43 :  0.6908382523059845\n",
      "44 :  0.677231410741806\n",
      "45 :  0.6551067411899567\n",
      "46 :  0.6113385009765625\n",
      "47 :  0.6110958379507064\n",
      "48 :  0.5867102122306824\n",
      "49 :  0.6097803670167923\n",
      "50 :  0.6456600040197372\n",
      "51 :  0.6364702624082565\n",
      "52 :  0.6093347406387329\n",
      "53 :  0.565988222360611\n",
      "54 :  0.6048157703876496\n",
      "55 :  0.601222054362297\n",
      "56 :  0.5918167722225189\n",
      "57 :  0.6086509346961975\n",
      "58 :  0.5512658554315567\n",
      "59 :  0.5782319849729538\n",
      "60 :  0.5425229471921921\n",
      "61 :  0.5999680119752884\n",
      "62 :  0.520405541062355\n",
      "63 :  0.5519966727495194\n",
      "64 :  0.548194864988327\n",
      "65 :  0.5257444173097611\n",
      "66 :  0.5302332681417465\n",
      "67 :  0.5380490809679032\n",
      "68 :  0.5612319684028626\n",
      "69 :  0.49896359860897066\n",
      "70 :  0.504838142991066\n",
      "71 :  0.5066097599267959\n",
      "72 :  0.487974956035614\n",
      "73 :  0.520405433177948\n",
      "74 :  0.5021174222230911\n",
      "75 :  0.5123160201311111\n",
      "76 :  0.513611251115799\n",
      "77 :  0.5203106021881103\n",
      "78 :  0.5040668600797653\n",
      "79 :  0.498113357424736\n",
      "80 :  0.4922244381904602\n",
      "81 :  0.5024084562063217\n",
      "82 :  0.48739304721355436\n",
      "83 :  0.4698552924394608\n",
      "84 :  0.4722380405664444\n",
      "85 :  0.4665832132101059\n",
      "86 :  0.46350103735923764\n",
      "87 :  0.4935159927606583\n",
      "88 :  0.4678842294216156\n",
      "89 :  0.45569792211055754\n",
      "90 :  0.4406594216823578\n",
      "91 :  0.4734927487373352\n",
      "92 :  0.4699856823682785\n",
      "93 :  0.4391833800077438\n",
      "94 :  0.4406178271770477\n",
      "95 :  0.44558353543281554\n",
      "96 :  0.4318369972705841\n",
      "97 :  0.4441045218706131\n",
      "98 :  0.44561542391777037\n",
      "99 :  0.4393499165773392\n",
      "100 :  0.4315763401985169\n",
      "101 :  0.45514901757240295\n",
      "102 :  0.438817053437233\n",
      "103 :  0.439236781001091\n",
      "104 :  0.42727783381938933\n",
      "105 :  0.4233481919765472\n",
      "106 :  0.4469039303064346\n",
      "107 :  0.4299641901254654\n",
      "108 :  0.4196392297744751\n",
      "109 :  0.4112265783548355\n",
      "110 :  0.41473507225513456\n",
      "111 :  0.4278632813692093\n",
      "112 :  0.41629596769809724\n",
      "113 :  0.42131932854652404\n",
      "114 :  0.42713498532772065\n",
      "115 :  0.41639955520629884\n",
      "116 :  0.4143634301424026\n",
      "117 :  0.4077293574810028\n",
      "118 :  0.4008253711462021\n",
      "119 :  0.42776782214641573\n",
      "120 :  0.4099084705114365\n",
      "121 :  0.39603173077106474\n",
      "122 :  0.4006658786535263\n",
      "123 :  0.4092725670337677\n",
      "124 :  0.40104385137557985\n",
      "125 :  0.39915507674217227\n",
      "126 :  0.4075592648983002\n",
      "127 :  0.40877396702766416\n",
      "128 :  0.40065761029720304\n",
      "129 :  0.4121993225812912\n",
      "130 :  0.3970957905054092\n",
      "131 :  0.43700299263000486\n",
      "132 :  0.39550999701023104\n",
      "133 :  0.3928121912479401\n",
      "134 :  0.3929780071973801\n",
      "135 :  0.3931892389059067\n",
      "136 :  0.39304808735847474\n",
      "137 :  0.37381056368350984\n",
      "138 :  0.3825365650653839\n",
      "139 :  0.3948493754863739\n",
      "140 :  0.39385891079902646\n",
      "141 :  0.3833706033229828\n",
      "142 :  0.37966243028640745\n",
      "143 :  0.38763716638088225\n",
      "144 :  0.37466178715229037\n",
      "145 :  0.3868816357851028\n",
      "146 :  0.3836318397521973\n",
      "147 :  0.3773836773633957\n",
      "148 :  0.3906294536590576\n",
      "149 :  0.372387683391571\n",
      "150 :  0.379415283203125\n",
      "151 :  0.3819911366701126\n",
      "152 :  0.385586154460907\n",
      "153 :  0.3734300780296326\n",
      "154 :  0.3778642475605011\n",
      "155 :  0.376362771987915\n",
      "156 :  0.39130266845226286\n",
      "157 :  0.375987708568573\n",
      "158 :  0.3722049790620804\n",
      "159 :  0.3661776751279831\n",
      "160 :  0.37768635094165803\n",
      "161 :  0.3747994041442871\n",
      "162 :  0.35552294850349425\n",
      "163 :  0.3649193829298019\n",
      "164 :  0.38306734144687654\n",
      "165 :  0.3712178701162338\n",
      "166 :  0.37491107642650606\n",
      "167 :  0.36184860646724704\n",
      "168 :  0.3819888472557068\n",
      "169 :  0.3773878216743469\n",
      "170 :  0.3755767625570297\n",
      "171 :  0.3610454821586609\n",
      "172 :  0.35604825854301453\n",
      "173 :  0.37050177574157717\n",
      "174 :  0.35968568444252014\n",
      "175 :  0.36594956398010253\n",
      "176 :  0.36835784077644346\n",
      "177 :  0.3732659375667572\n",
      "178 :  0.35935851752758025\n",
      "179 :  0.3614794635772705\n",
      "180 :  0.353885617852211\n",
      "181 :  0.35804946660995485\n",
      "182 :  0.3569996851682663\n",
      "183 :  0.37045551896095275\n",
      "184 :  0.3614399617910385\n",
      "185 :  0.36030291676521303\n",
      "186 :  0.352035768032074\n",
      "187 :  0.3514384025335312\n",
      "188 :  0.34820178389549256\n",
      "189 :  0.3540548318624496\n",
      "190 :  0.35282599806785586\n",
      "191 :  0.353961306810379\n",
      "192 :  0.337663459777832\n",
      "193 :  0.34649787306785584\n",
      "194 :  0.3479478961229324\n",
      "195 :  0.34396723508834837\n",
      "196 :  0.35265316426753995\n",
      "197 :  0.3592027175426483\n",
      "198 :  0.34778655171394346\n",
      "199 :  0.3561398762464523\n",
      "200 :  0.3379571145772934\n",
      "201 :  0.3499202984571457\n",
      "202 :  0.352954323887825\n",
      "203 :  0.3502741014957428\n",
      "204 :  0.3515367925167084\n",
      "205 :  0.36151672542095187\n",
      "206 :  0.3474379312992096\n",
      "207 :  0.34360240638256073\n",
      "208 :  0.3476384341716766\n",
      "209 :  0.3366382497549057\n",
      "210 :  0.34113870322704315\n",
      "211 :  0.34790264070034027\n",
      "212 :  0.3355782014131546\n",
      "213 :  0.33565973162651064\n",
      "214 :  0.3457837301492691\n",
      "215 :  0.33505435824394225\n",
      "216 :  0.33073519468307494\n",
      "217 :  0.33993342697620393\n",
      "218 :  0.33190453588962554\n",
      "219 :  0.3474498039484024\n",
      "220 :  0.3451566070318222\n",
      "221 :  0.3426289814710617\n",
      "222 :  0.3333173471689224\n",
      "223 :  0.33885189771652224\n",
      "224 :  0.33363635540008546\n",
      "225 :  0.34061968386173247\n",
      "226 :  0.33255468249320985\n",
      "227 :  0.3388475805521011\n",
      "228 :  0.3356612861156464\n",
      "229 :  0.3351188188791275\n",
      "230 :  0.3455777853727341\n",
      "231 :  0.3343436831235886\n",
      "232 :  0.33994822680950165\n",
      "233 :  0.3265781688690186\n",
      "234 :  0.32393580257892607\n",
      "235 :  0.3405782288312912\n",
      "236 :  0.34449969172477724\n",
      "237 :  0.3346690547466278\n",
      "238 :  0.33513745069503786\n",
      "239 :  0.33333363056182863\n",
      "240 :  0.33872750341892244\n",
      "241 :  0.3409885263442993\n",
      "242 :  0.3305621588230133\n",
      "243 :  0.32831686913967134\n",
      "244 :  0.32850841760635374\n",
      "245 :  0.3290781706571579\n",
      "246 :  0.3253487634658814\n",
      "247 :  0.3332730787992477\n",
      "248 :  0.32624429166316987\n",
      "249 :  0.3247594052553177\n",
      "250 :  0.33017615556716917\n",
      "251 :  0.3260532993078232\n",
      "252 :  0.33718794465065005\n",
      "253 :  0.3297793024778366\n",
      "254 :  0.32440438389778137\n",
      "255 :  0.33505921959877016\n",
      "256 :  0.32992649734020235\n",
      "257 :  0.32280031323432923\n",
      "258 :  0.3271308821439743\n",
      "259 :  0.33212882936000826\n",
      "260 :  0.3307351839542389\n",
      "261 :  0.331047745347023\n",
      "262 :  0.3270221090316772\n",
      "263 :  0.329511958360672\n",
      "264 :  0.32661991715431216\n",
      "265 :  0.33276353538036346\n",
      "266 :  0.3303490275144577\n",
      "267 :  0.32098366498947145\n",
      "268 :  0.3188045626878738\n",
      "269 :  0.32537592709064483\n",
      "270 :  0.32107274651527407\n",
      "271 :  0.3193497782945633\n",
      "272 :  0.32368050813674926\n",
      "273 :  0.31928636252880094\n",
      "274 :  0.3269601660966873\n",
      "275 :  0.3278907859325409\n",
      "276 :  0.32793869376182555\n",
      "277 :  0.32689041197299956\n",
      "278 :  0.3185531568527222\n",
      "279 :  0.323825963139534\n",
      "280 :  0.3230631470680237\n",
      "281 :  0.329103017449379\n",
      "282 :  0.3152976107597351\n",
      "283 :  0.31841778874397275\n",
      "284 :  0.30980714976787566\n",
      "285 :  0.3258757698535919\n",
      "286 :  0.31754240810871126\n",
      "287 :  0.3236097592115402\n",
      "288 :  0.31174268007278444\n",
      "289 :  0.3204535710811615\n",
      "290 :  0.3219874507188797\n",
      "291 :  0.31716536581516264\n",
      "292 :  0.31958474814891813\n",
      "293 :  0.3210949182510376\n",
      "294 :  0.32352949380874635\n",
      "295 :  0.3223466795682907\n",
      "296 :  0.31645576417446136\n",
      "297 :  0.3216755437850952\n",
      "298 :  0.31562899112701415\n",
      "299 :  0.3217413365840912\n",
      "300 :  0.3119777935743332\n",
      "301 :  0.314566633105278\n",
      "302 :  0.32038638114929197\n",
      "303 :  0.3129955404996872\n",
      "304 :  0.3211617308855057\n",
      "305 :  0.31461358547210694\n",
      "306 :  0.3201799339056015\n",
      "307 :  0.3100652676820755\n",
      "308 :  0.3128540015220642\n",
      "309 :  0.315059517621994\n",
      "310 :  0.31075474560260774\n",
      "311 :  0.3118218010663986\n",
      "312 :  0.31526736438274383\n",
      "313 :  0.3165449285507202\n",
      "314 :  0.31525549829006194\n",
      "315 :  0.3074211543798447\n",
      "316 :  0.32115009844303133\n",
      "317 :  0.31713585674762723\n",
      "318 :  0.3224054455757141\n",
      "319 :  0.31760833084583284\n",
      "320 :  0.30724339246749877\n",
      "321 :  0.31953088760375975\n",
      "322 :  0.3167658090591431\n",
      "323 :  0.314679217338562\n",
      "324 :  0.31938418209552766\n",
      "325 :  0.30499730944633485\n",
      "326 :  0.3086947321891785\n",
      "327 :  0.3134482753276825\n",
      "328 :  0.31160444021224976\n",
      "329 :  0.31107290148735045\n",
      "330 :  0.3116470569372177\n",
      "331 :  0.3115574580430984\n",
      "332 :  0.3187216740846634\n",
      "333 :  0.3186609447002411\n",
      "334 :  0.3132514637708664\n",
      "335 :  0.31738294899463654\n",
      "336 :  0.3132506364583969\n",
      "337 :  0.3111091363430023\n",
      "338 :  0.31721511721611023\n",
      "339 :  0.31115334391593935\n",
      "340 :  0.31455316781997683\n",
      "341 :  0.31667392790317533\n",
      "342 :  0.3163036459684372\n",
      "343 :  0.31675951719284057\n",
      "344 :  0.30548967838287355\n",
      "345 :  0.31110944867134094\n",
      "346 :  0.32158384919166566\n",
      "347 :  0.3128298717737198\n",
      "348 :  0.3123284024000168\n",
      "349 :  0.3109629875421524\n",
      "350 :  0.31036401212215425\n",
      "351 :  0.3096424001455307\n",
      "352 :  0.3082825523614883\n",
      "353 :  0.30690957069396974\n",
      "354 :  0.30661240220069885\n",
      "355 :  0.31547633588314056\n",
      "356 :  0.3111415618658066\n",
      "357 :  0.3159503322839737\n",
      "358 :  0.3118835061788559\n",
      "359 :  0.3128183513879776\n",
      "360 :  0.3097732186317444\n",
      "361 :  0.31286359012126924\n",
      "362 :  0.3129448926448822\n",
      "363 :  0.30909577071666716\n",
      "364 :  0.30962093114852907\n",
      "365 :  0.3070177084207535\n",
      "366 :  0.3065260326862335\n",
      "367 :  0.3101063126325607\n",
      "368 :  0.30790497839450837\n",
      "369 :  0.2995039504766464\n",
      "370 :  0.30618171632289887\n",
      "371 :  0.30705520391464236\n",
      "372 :  0.3022058606147766\n",
      "373 :  0.30610600173473357\n",
      "374 :  0.3053875774145126\n",
      "375 :  0.3036564886569977\n",
      "376 :  0.3083772766590118\n",
      "377 :  0.31440847158432006\n",
      "378 :  0.30722725093364717\n",
      "379 :  0.30295021533966066\n",
      "380 :  0.3117779386043549\n",
      "381 :  0.3062985050678253\n",
      "382 :  0.30775135695934297\n",
      "383 :  0.30990965843200685\n",
      "384 :  0.3030556374788284\n",
      "385 :  0.3044029778242111\n",
      "386 :  0.30515784561634063\n",
      "387 :  0.30376065194606783\n",
      "388 :  0.3184183096885681\n",
      "389 :  0.30540911078453065\n",
      "390 :  0.3020153224468231\n",
      "391 :  0.3081011551618576\n",
      "392 :  0.30659600377082824\n",
      "393 :  0.31058374762535096\n",
      "394 :  0.30646432757377623\n",
      "395 :  0.3082183623313904\n",
      "396 :  0.3005551236867905\n",
      "397 :  0.30298766791820525\n",
      "398 :  0.3055181968212128\n",
      "399 :  0.3048605364561081\n",
      "400 :  0.3066767305135727\n",
      "401 :  0.30023125529289246\n",
      "402 :  0.2993979239463806\n",
      "403 :  0.3090645617246628\n",
      "404 :  0.3001212066411972\n",
      "405 :  0.30535572350025175\n",
      "406 :  0.30643924057483674\n",
      "407 :  0.30904479563236237\n",
      "408 :  0.30687863647937774\n",
      "409 :  0.30297449350357053\n",
      "410 :  0.3019079613685608\n",
      "411 :  0.30905852794647215\n",
      "412 :  0.30635309100151065\n",
      "413 :  0.30868974566459656\n",
      "414 :  0.3001001274585724\n",
      "415 :  0.31318058669567106\n",
      "416 :  0.30842764258384703\n",
      "417 :  0.30415615558624265\n",
      "418 :  0.30296737313270566\n",
      "419 :  0.2981612718105316\n",
      "420 :  0.3032216554880142\n",
      "421 :  0.30371171176433565\n",
      "422 :  0.30083016216754915\n",
      "423 :  0.301555352807045\n",
      "424 :  0.3068571084737778\n",
      "425 :  0.3028121066093445\n",
      "426 :  0.3014314639568329\n",
      "427 :  0.3068785780668259\n",
      "428 :  0.30873936831951143\n",
      "429 :  0.30202221632003784\n",
      "430 :  0.3005677568912506\n",
      "431 :  0.30293144166469577\n",
      "432 :  0.30093817293643954\n",
      "433 :  0.30887348353862765\n",
      "434 :  0.30306564331054686\n",
      "435 :  0.3034737730026245\n",
      "436 :  0.29739804327487945\n",
      "437 :  0.3024435323476791\n",
      "438 :  0.2998389285802841\n",
      "439 :  0.29652871310710904\n",
      "440 :  0.2990643590688705\n",
      "441 :  0.3095337480306625\n",
      "442 :  0.3025940376520157\n",
      "443 :  0.3021788626909256\n",
      "444 :  0.3024949741363525\n",
      "445 :  0.3074883109331131\n",
      "446 :  0.3011939924955368\n",
      "447 :  0.3034566766023636\n",
      "448 :  0.30041047513484953\n",
      "449 :  0.2999567914009094\n",
      "450 :  0.2998132884502411\n",
      "451 :  0.2979920184612274\n",
      "452 :  0.3020323395729065\n",
      "453 :  0.3020318078994751\n",
      "454 :  0.29869220674037933\n",
      "455 :  0.3062980389595032\n",
      "456 :  0.3038505429029465\n",
      "457 :  0.3013742560148239\n",
      "458 :  0.29927231252193454\n",
      "459 :  0.3103558152914047\n",
      "460 :  0.30009986758232116\n",
      "461 :  0.30323230028152465\n",
      "462 :  0.3040418928861618\n",
      "463 :  0.30390640020370485\n",
      "464 :  0.3007181042432785\n",
      "465 :  0.3032683742046356\n",
      "466 :  0.30361185610294344\n",
      "467 :  0.3022297489643097\n",
      "468 :  0.3022816926240921\n",
      "469 :  0.30113498389720916\n",
      "470 :  0.3038751202821732\n",
      "471 :  0.30009634256362916\n",
      "472 :  0.3032864159345627\n",
      "473 :  0.30441515922546386\n",
      "474 :  0.29620504677295684\n",
      "475 :  0.30368989288806914\n",
      "476 :  0.3012795782089233\n",
      "477 :  0.3012276840209961\n",
      "478 :  0.3022840315103531\n",
      "479 :  0.29725122392177583\n",
      "480 :  0.30447168290615084\n",
      "481 :  0.3042066359519959\n",
      "482 :  0.29984224021434785\n",
      "483 :  0.30553834497928617\n",
      "484 :  0.3009692859649658\n",
      "485 :  0.30124669551849365\n",
      "486 :  0.2928357398509979\n",
      "487 :  0.30316813945770266\n",
      "488 :  0.29962059915065764\n",
      "489 :  0.30220861196517945\n",
      "490 :  0.298244840502739\n",
      "491 :  0.2994936501979828\n",
      "492 :  0.2951124727725983\n",
      "493 :  0.2981271594762802\n",
      "494 :  0.2990995740890503\n",
      "495 :  0.29481711506843566\n",
      "496 :  0.298460590839386\n",
      "497 :  0.29918602883815765\n",
      "498 :  0.29884423851966857\n",
      "499 :  0.2976905262470245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m T5Trainer()\n",
      "Cell \u001b[0;32mIn[9], line 77\u001b[0m, in \u001b[0;36mT5Trainer\u001b[0;34m(output_dir)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstarting training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(model_params[\u001b[39m\"\u001b[39m\u001b[39mTRAIN_EPOCHS\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m---> 77\u001b[0m     train(epoch, tokenizer, model, device, training_loader, optimizer)\n\u001b[1;32m     78\u001b[0m     predictions, actuals,sources \u001b[39m=\u001b[39m validate(epoch, tokenizer, model, device, val_loader)\n\u001b[1;32m     79\u001b[0m     final_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mGenerated Text\u001b[39m\u001b[39m\"\u001b[39m: predictions, \u001b[39m\"\u001b[39m\u001b[39mActual Text\u001b[39m\u001b[39m\"\u001b[39m: actuals,\u001b[39m\"\u001b[39m\u001b[39mSource Text\u001b[39m\u001b[39m\"\u001b[39m:sources})\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, tokenizer, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m     total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m( \u001b[39mint\u001b[39m(i\u001b[39m/\u001b[39m\u001b[39m50\u001b[39m), \u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m, aver_loss)\n\u001b[0;32m---> 28\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T5Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dogyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "484cb91c4aa6ea064c5710379ab95cfbca66410bd583213d13aabef9da2f59e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
